0:00 | ANNOUNCER: Open content is provided under a creative
0:02 | commons license.
0:03 | Your support will help MIT OpenCourseWare continue to
0:06 | offer high-quality educational resources for free.
0:10 | To make a donation, or view additional materials from
0:13 | hundreds of MIT courses, visit MIT OpenCourseWare at
0:17 | ocw.mit.edu .
0:19 | PROFESSOR ERIC GRIMSON: Last time, we ended up, we sort of
0:23 | did this tag team thing, Professor Guttag did the first
0:25 | half, I did the second half of the lecture, and the second
0:27 | half of the lecture, we started talking about
0:29 | complexity.
0:30 | Efficiency.
0:32 | Orders of growth.
0:33 | And that's what we're going to spend today on, is talking
0:35 | about that topic.
0:36 | I'm going to use it to build over the
0:37 | next couple of lectures.
0:39 | I want to remind you that we were talking at a fairly high
0:41 | level about complexity.
0:42 | We're going to get down into the weeds in a second here.
0:45 | But the things we were trying to stress were that it's an
0:47 | important design decision, when you are coming up with a
0:51 | piece of code, as to what kind of efficiency your code has.
0:54 | And the second thing that we talked about is this idea that
0:58 | we want you to in fact learn how to relate a choice you
1:00 | make about a piece of code to what the
1:03 | efficiency is going to be.
1:05 | So in fact, over the next thirty or forty minutes, we're
1:07 | going to show you a set of examples of sort of canonical
1:10 | algorithms, and the different classes of complexity.
1:13 | Because one of the things that you want to do as a good
1:15 | designer is to basically map a new problem
1:19 | into a known domain.
1:20 | You want to take a new problem and say, what does
1:23 | this most look like?
1:24 | What is the class of algorithm that's-- that probably applies
1:27 | to this, and how do I pull something out of that, if you
1:30 | like, a briefcase of possible algorithms to solve?
1:33 | All right, having said that, let's do some examples.
1:36 | I'm going to show you a sequence of algorithms,
1:39 | they're mostly simple algorithms, that's OK.
1:41 | But I want you to take away from this how we reason about
1:44 | the complexity of these algorithms. And I'll remind
1:47 | you, we said we're going to mostly talk about time.
1:48 | We're going to be counting the number of basic steps it takes
1:51 | to solve the problem.
1:53 | So here's the first example I want to do.
1:55 | I'm going to write a function to compute integer power
1:59 | exponents. a to the b where b is an integer.
2:02 | And I'm going to do it only using multiplication and
2:05 | addition and some simple tests.
2:07 | All right?
2:07 | And yeah, I know it comes built in, that's OK, what we
2:10 | want to do is use it as an example to look at it.
2:12 | So I'm going to build something that's going to do
2:20 | iterative exponentiation.
2:22 | OK?
2:23 | And in fact, if you look at the code up here, and it's on
2:26 | your handout, the very first one, x 1, right here-- if I
2:30 | could ask you to look at it-- is a piece of code to do it.
2:33 | And I'm less interested in the code than how we're going to
2:35 | analyze it, but let's look at it for a second.
2:37 | All right, you can see that this little piece of code,
2:42 | it's got a loop in there, and what's it doing?
2:45 | It's basically cycling through the loop,
2:47 | multiplying by a each time.
2:49 | So first time through the loop, the answer is 1.
2:51 | Second time it-- sorry, as it enters the loop, at the time
2:53 | it enter-- exits, the answer is a.
2:56 | Next time through the loop it goes to a squared.
2:58 | Next time through the loop it goes to a cubed.
2:59 | And it's just gathering together the multiplications
3:02 | while counting down the exponent.
3:05 | And you can see it when we get down to the end test here,
3:07 | we're going to pop out of there and we're going to
3:08 | return the answer.
3:11 | I could run it, it'll do the right thing.
3:13 | What I want to think about though, is, how much
3:16 | time does this take?
3:17 | How many steps does it take for this function to run?
3:22 | Well, you can kind of look at it, right?
3:23 | The key part of that is that WHILE loop.
3:26 | And what are the steps I want to count?
3:27 | They're inside that loop--
3:28 | I've got the wrong glasses so I'm going to have to squint--
3:30 | and we've got one test which is a comparison, we've got
3:33 | another test which is a multiplication-- sorry, not a
3:36 | test, we've got another step which is a multiplication--
3:38 | and another step that is a subtraction.
3:41 | So each time through the loop, I'm doing three steps.
3:44 | Three basic operations.
3:46 | How many times do I go through the loop?
3:48 | Somebody help me out.
3:51 | Hand up?
3:51 | Sorry. b times.
3:53 | You're right.
3:53 | Because I keep counting down each time around-- mostly I've
3:56 | got to unload this candy, which is driving me nuts, so--
3:59 | thank you. b times.
4:00 | So I've got to go 3 b steps.
4:03 | All right, I've got to go through the loop b times, I've
4:05 | got three steps each time, and then when I pop out of the
4:07 | loop, I've got two more steps.
4:09 | All right, I've got the initiation of answer and the
4:12 | return of it.
4:12 | So I take 2 plus 3 b steps to go through this loop.
4:19 | OK.
4:19 | So if b is 300, it takes 902 steps. b is 3000, it takes
4:27 | 9002 steps. b is 30,000 you get the point, it
4:31 | takes 90,002 steps.
4:35 | OK.
4:35 | So the point here is, first of all, I can count these things,
4:38 | but the second thing you can see is, as the size of the
4:41 | problems get larger, that additive constant, that 2,
4:46 | really doesn't matter.
4:47 | All right?
4:47 | The difference between 90,000 steps and 90,002 steps, who
4:51 | cares about the 2, right?
4:53 | So, and typically, we're not going to worry about those
4:55 | additive constants.
4:56 | The second one is, this multiplicative constant here
5:00 | is 3, in some sense also isn't all that crucial.
5:04 | Does it really matter to you whether your code is going to
5:06 | take 300 years or 900 years to run?
5:10 | Problem is, how big is that number?
5:11 | So we're going to typically also not worry about the
5:14 | multiplicative constants.
5:15 | This factor here.
5:18 | What we really want to worry about is, as the size of the
5:21 | problem gets larger, how does this thing grow?
5:25 | How does the cost go up?
5:27 | And so what we're going to primarily talk about as a
5:29 | consequence is the rate of growth as the size of the
5:46 | problem grows.
5:47 | If it was, how much bigger does this get as I make the
5:53 | problem bigger?
5:55 | And what that really says is, that we're going to use this
5:57 | using something we're going to just
5:58 | call asymptotic notation--
6:02 | I love spelling this word-- meaning, as in the limit as
6:08 | the size of the problem gets bigger, how do I characterize
6:11 | this growth?
6:12 | All right?
6:13 | You'll find out, if you go on to some of the other classes
6:15 | in course 6, there are a lot of different ways that you can
6:18 | measure this.
6:18 | The most common one, and the one we're going to use, is
6:21 | what's often called big Oh notation.
6:27 | This isn't big Oh as in, oh my God I'm shocked the markets
6:30 | are collapsing, This is called big Oh because we use the
6:33 | Greek letter, capital letter, omicron to represent it.
6:35 | And the way we're going to do this, or what this represents,
6:38 | let me write this carefully for you, big Oh notation is
6:41 | basically going to be an upper limit to the growth of a
6:53 | function as the input grow-- as the input gets large.
7:00 | Now we're going to see a bunch of examples, and I know those
7:06 | are words, let me give you an example.
7:08 | I would write f of x is in big Oh of n squared.
7:16 | And what does it say?
7:17 | It says that function, f of x, is bounded above, there's an
7:21 | upper limit on it, that this grows no faster than quadratic
7:25 | in n, n squared.
7:28 | OK.
7:29 | And first of all, you say, wait a minute, x and n?
7:31 | Well, one of the things we're going to see is x is the input
7:34 | to this particular problem, n is a measure of the size of x.
7:38 | And we're going to talk about how we come up with that. n
7:42 | measures the size of x.
7:46 | OK.
7:47 | In this example I'd use b.
7:50 | All right, as b get-- b is the thing that's changing as I go
7:52 | along here, but it could be things like, how many elements
7:54 | are there in a list if the input is a list, could be how
7:56 | many digits are there in a string if the input's a
7:58 | string, it could be the size of the integer as we go along.
8:01 | All right.?
8:02 | And what we want to do then, is we want to basically come
8:05 | up with, how do we characterize the growth--
8:09 | God bless you-- of this problem in terms of this
8:11 | quadra-- sorry, terms of this exponential growth
8:14 | Now, one last piece of math.
8:16 | I could cheat.
8:17 | I said I just want an upper bound.
8:19 | I could get a really big upper bound, this thing grows
8:21 | exponentially.
8:22 | That doesn't help me much.
8:23 | Usually what I want to talk about is what's the smallest
8:26 | size class in which this function grows?
8:30 | With all of that, what that says, is that this we would
8:32 | write is order b.
8:35 | That algorithm is linear.
8:40 | You can see it.
8:41 | I've said the product was is 2 plus 3 b.
8:44 | As I make b really large, how does this thing grow?
8:47 | It grows as b.
8:48 | The 3 doesn't matter, it's just a constant,
8:50 | it's growing linearly.
8:51 | Another way of saying it is, if I, for example, increase
8:55 | the size of the input by 10, the amount of time
8:59 | increases by 10.
9:00 | And that's a sign that it's linear.
9:03 | OK.
9:04 | So there's one quick example.
9:05 | Let's look at another example.
9:07 | If you look at x 2, this one right here in your handout.
9:12 | OK.
9:12 | This is another way of doing exponentiation, but this one's
9:17 | a recursive function.
9:18 | All right?
9:18 | So again, let's look at it.
9:20 | What does it say to do?
9:21 | Well, it's basically saying a similar thing.
9:23 | It says, if I am in the base case, if b is equal to 1, the
9:26 | answer is just a.
9:27 | I could have used if b is equal to 0, the answer is 1,
9:30 | that would have also worked.
9:31 | Otherwise, what do I say?
9:33 | I say, ah, I'm in a nice recursive way, a to the b is
9:36 | the same as a times a to the b minus 1.
9:41 | And I've just reduced that problem to a simpler version
9:43 | of the same problem.
9:45 | OK, and you can see that this thing ought to unwrap, it's
9:47 | going to keep extending out those multiplications until
9:49 | gets down to the base case, going to
9:51 | collapse them all together.
9:53 | OK.
9:53 | Now I want to know what's the order of growth here?
9:57 | What's the complexity of this?
10:00 | Well, gee.
10:01 | It looks like it's pretty straightforward, right?
10:04 | I've got one test there, and then I've just got one thing
10:06 | to do here, which has got a subtraction and a
10:09 | multiplication.
10:10 | Oh, but how do I know how long it takes to do x 2?
10:14 | All right, we were counting basic steps.
10:16 | We don't know how long it takes to do x 2.
10:19 | So I'm going to show you a little trick for
10:21 | figuring that out.
10:23 | And in particular, I'm going to cheat slightly, I'm going
10:25 | to use a little bit of abusive mathematics, but I'm going to
10:28 | show you a trick to figure it out.
10:30 | In the case of a recursive exponentiator, I'm going to do
10:36 | the following trick.
10:36 | I'm going to let t of b be the number of steps it takes to
10:41 | solve the problem of size b.
10:43 | OK, and I can figure this out.
10:45 | I've got one test, I've got a subtraction, I've got a
10:49 | multiplication, that's three steps, plus whatever number of
10:55 | steps it takes to solve a problem of size b minus 1.
10:58 | All right, this is what's called a recurrence relation,
11:02 | there are actually cool ways to solve them.
11:05 | We can kind of eyeball it.
11:07 | In particular, how would I write an expression for
11:10 | t of b minus 1?
11:12 | Well the same way.
11:13 | This is 3 plus 3 plus t of b minus 2.
11:18 | Right?
11:18 | I'm using exactly the same form to reduce this.
11:21 | You know, you can see what's going to happen.
11:23 | If I reduce that, it would be 3 plus t of b minus 3, so in
11:26 | general, this is 3 k plus t of b minus k.
11:33 | OK.
11:33 | I'm just expanding it out.
11:36 | When am I done?
11:38 | How do I stop this?
11:40 | Any suggestions?
11:43 | Don't you hate it when professors ask questions?
11:45 | Yeah.
11:48 | Actually, I think I want b minus k equal to 1.
11:52 | Right?
11:52 | When this gets down to t of 1, I'm in the base case.
11:55 | So I'm done when b minus k equals 1, or k
12:01 | equals b minus 1.
12:04 | Right, that gets me down to the base case, I'm solving a
12:05 | problem with size 1, and in that case, I've got two more
12:08 | operations to do, so I plug this all back in, I-- t of b
12:12 | is I'm going to put k for b minus 1 I get 3 b minus 1 plus
12:19 | t of 1, so t of 1 is 2, so this is 3 b minus 1 plus 2, or
12:26 | 3 b minus 1.
12:30 | OK.
12:30 | A whole lot of work to basically say,
12:34 | again, order b is linear.
12:38 | But that's also nice, it lets you see how the recursive
12:41 | thing is simply unwrapping but the complexity in terms of the
12:43 | amount of time it takes is going to be the same.
12:46 | I owe you a candy.
12:48 | Thank you.
12:50 | OK.
12:51 | At this point, if we stop, you'll think all algorithms
12:53 | are linear.
12:53 | This is really boring.
12:55 | But they're not.
12:56 | OK?
12:56 | So let me show you another way I could do exponentiation.
13:00 | Taking an advantage of a trick.
13:01 | I want to solve a to the b.
13:08 | Here's another way I could do that.
13:10 | OK.
13:10 | If b is even, then a to the b is the same as a squared all
13:23 | to the b over 2.
13:24 | All right, just move the 2's around.
13:26 | It's the same thing.
13:27 | You're saying, OK, so what?
13:29 | Well gee, notice.
13:31 | This is a primitive operation.
13:33 | That's a primitive operation.
13:35 | But in one step, I've reduced this problem in half.
13:38 | I didn't just make it one smaller, I
13:40 | made it a half smaller.
13:41 | That's a nice deal.
13:43 | OK.
13:44 | But I'm not always going to have b as even.
13:45 | If b is odd, what do I do?
13:47 | Well, go back to what I did before.
13:56 | Multiply a by a to the b minus 1.
13:59 | You know, that's nice, right?
14:00 | Because if b was odd, then b minus one is even, which means
14:03 | on the next step, I can cut the problem in half again.
14:07 | OK?
14:08 | All right. x 3, as you can see right here, does exactly that.
14:17 | OK?
14:17 | You can take a quick look at it, even with the wrong
14:20 | glasses on, it says if a-- sorry, b is equal to 1, I'm
14:22 | just going to return a.
14:24 | Otherwise there's that funky little test. I'll do the
14:26 | remainder multiplied by 2, because these are integers,
14:29 | that gives me back an integer, I just check to see if it's
14:31 | equal to b, that tells me whether it's even or odd.
14:33 | And in the even case, I'd square, divide by half, call
14:38 | this again: in the odd case, I go b minus 1 and
14:41 | then multiply by a.
14:44 | I'll let you chase it through, it does work.
14:46 | What I want to look at is, what's the
14:48 | order of growth here?
14:51 | This is a little different, right?
14:52 | It's going to take a little bit more work, so let's see if
14:54 | we can do it.
14:58 | In the b even case, again I'm going to let t of b be the
15:01 | number of steps I want to go through.
15:03 | And we can kind of eyeball this thing, right?
15:05 | If b is even, I've got a test to see if b is equal to 1, and
15:09 | then I've got to do the remainder, the multiplication,
15:11 | and the test, I'm up to four.
15:14 | And then in the even case, I've got to do a square and
15:17 | the divide.
15:17 | So I've got six steps, plus whatever it takes to solve the
15:24 | problem size b over 2, right?
15:26 | Because that's the recursive call. b as odd, well I can go
15:33 | through the same kind of thing.
15:34 | I've got the same first four steps, I've got a check to see
15:37 | is it 1, I got a check to see if it's even, and then in the
15:40 | odd case, I've got to subtract 1 from b, that's a fifth step,
15:43 | I've got to go off and solve the recursive problem, and
15:45 | then I'm going to do one more multiplication, so it's 6
15:48 | plus, in this case, t of b minus 1.
15:52 | Because it's now solving a one-smaller problem.
15:55 | On the next step though, this, we get substituted by that.
16:01 | Right, on the next step, I'm back in the even case, it's
16:03 | going to take six more steps, plus t of b minus 1.
16:09 | Oops, sorry about that, over 2.
16:12 | Because b minus 1 is now even.
16:15 | Don't sweat the details here, I just want you to see the
16:17 | reason it goes through it.
16:17 | What I now have, though, is a nice thing.
16:19 | It says, in either case, in general, t of b-- and this is
16:25 | where I'm going to abuse notation a little bit-- but I
16:27 | can basically bound it by t, 12 steps plus t of b over 2.
16:33 | And the abuse is, you know, it's not quite right, it
16:35 | depends upon whether it's all ready, but you can see in
16:36 | either case, after 12 steps, 2 runs through this and down to
16:39 | a problem size b over 2.
16:41 | Why's that nice?
16:42 | Well, that then says after another 12 steps, we're down
16:49 | to a problem with size t of b over 4.
16:51 | And if I pull it out one more level, it's 12 plus 12 plus t
16:59 | of b over 8, which in general is going to be, after k steps,
17:04 | 12 k because I'll have 12 of those to add up, plus t of b
17:08 | over 2 to the k.
17:13 | When am I done?
17:15 | When do I get down to the base case?
17:20 | Somebody help me out.
17:21 | What am I looking for?
17:25 | Yeah.
17:25 | You're jumping slightly ahead of me, but basically, I'm done
17:28 | when this is equal to 1, right?
17:29 | Because I get down to the base case, so I'm done when b u is
17:32 | over 2 to the k is equal to 1, and you're absolutely right,
17:36 | that's when k is log base 2 of b.
17:44 | You're sitting a long ways back, I have no idea if I'll
17:46 | make it this far or not.
17:48 | Thank you.
17:50 | OK.
17:51 | There's some constants in there, but this
17:52 | is order log b.
17:57 | Logarithmic.
17:59 | This matters.
18:01 | This matters a lot.
18:02 | And I'm going to show you an example in a second, just to
18:03 | drive this home, but notice the characteristics.
18:06 | In the first two cases, the problem reduced
18:09 | by 1 at each step.
18:11 | Whether it was recursive or iterative.
18:13 | That's a sign that it's probably linear.
18:15 | This case, I reduced the size of the problem in half.
18:19 | It's a good sign that this is logarithmic, and I'm going to
18:21 | come back in a second to why logs are a great thing.
18:25 | Let me show you one more class, though, about-- sorry,
18:28 | let me show you two more classes of algorithms. Let's
18:30 | look at the next one g-- and there's a bug in your handout,
18:32 | it should be g of n and m, I apologize for that, I changed
18:37 | it partway through and didn't catch it.
18:40 | OK.
18:40 | Order of growth here.
18:45 | Anybody want to volunteer a guess?
18:47 | Other than the TAs, who know?
18:53 | OK.
18:54 | Let's think it through.
18:56 | I've got two loops.
18:57 | All right?
18:58 | We already saw with one of the loops, you know, it looked
19:01 | like it might be linear, depending on what's inside of
19:03 | it, but let's think about this.
19:04 | I got two loops with g.
19:06 | What's g do?
19:07 | I've got an initialization of x, and then I say, for i in
19:10 | the range, so that's basically from 0 up to n minus
19:12 | 1, what do I do?
19:14 | Well, inside of there, I've got another loop, for j in the
19:17 | range from 0 up to m minus 1.
19:20 | What's the complexity of that inner loop?
19:24 | Sorry?
19:26 | OK.
19:27 | You're doing the whole thing for me.
19:28 | What's the complexity just of this inner loop here?
19:30 | Just this piece.
19:31 | How many times do I go through that loop? m.
19:36 | Right?
19:36 | I'm going to get back to your answer in a second, because
19:39 | you're heading in the right direction.
19:40 | The inner loop, this part here, I do m times.
19:43 | There's one step inside of it.
19:44 | Right?
19:46 | How many times do I go through that loop?
19:48 | Ah, n times, because for each value of i, I'm going to do
19:53 | that m thing, so that is, close to what you said, right?
19:56 | The order complexity here, if I actually write it, would
19:59 | be-- sorry, order n times m, and if m was equal to n, that
20:08 | would be order n squared, and this is quadratic.
20:14 | And that's a different behavior.
20:19 | OK.
20:20 | What am I doing?
20:21 | Building up examples of algorithms. Again, I want you
20:24 | to start seeing how to map the characteristics of the code--
20:26 | the characteristics of the algorithm, let's not call it
20:28 | the code-- to the complexity.
20:29 | I'm going to come back to that in a second with that, but I
20:31 | need to do one more example, and I've got to use my
20:33 | high-tech really expensive props.
20:36 | Right.
20:38 | So here's the fourth or fifth, whatever we're up to, I guess
20:39 | fifth example.
20:41 | This is an example of a problem
20:42 | called Towers of Hanoi.
20:43 | Anybody heard about this problem?
20:44 | A few tentative hands.
20:47 | OK.
20:49 | Here's the story as I am told it.
20:51 | There's a temple in the middle of Hanoi.
20:53 | In that temple, there are three very large
20:56 | diamond-encrusted posts, and on those posts are sixty-four
21:00 | disks, all of a different size.
21:02 | And they're, you know, covered with jewels and all sorts of
21:05 | other really neat stuff.
21:07 | There are a set of priests in that temple, and their task is
21:10 | to move the entire stack of sixty-four disks from one post
21:15 | to a second post. When they do this, you know, the universe
21:19 | ends or they solve the financial crisis in Washington
21:21 | or something like that actually good happens, right?
21:24 | Boy, none of you have 401k's, you're not even wincing at
21:26 | that thing.
21:28 | All right.
21:29 | The rules, though, are, they can only move one disk at a
21:31 | time, and they can never cover up a smaller disk with a
21:36 | larger disk.
21:37 | OK.
21:38 | Otherwise you'd just move the whole darn stack, OK?
21:40 | So we want to solve that problem.
21:42 | We want to write a piece of code that helps these guys
21:43 | out, so I'm going to show you an example.
21:45 | Let's see if we can figure out how to do this.
21:47 | So, we'll start with the easy one.
21:49 | Moving a disk of size 1.
21:51 | OK, that's not so bad.
21:53 | Moving a stack of size 2, if I want to go there, I need to
21:55 | put this one temporarily over here so I can move the bottom
21:57 | one before I move it over.
22:00 | Moving a stack of size 3, again, if I want to go over
22:02 | there, I need to make sure I can put the spare one over
22:04 | here before I move the bottom one, I can't cover up any of
22:06 | the smaller ones with the larger one, but
22:08 | I can get it there.
22:09 | Stack of size 4, again I'm going there, so I'm going to
22:13 | do this initially, no I'm not, I'm going to start again.
22:15 | I'm going to go there initially, so I can move this
22:17 | over here, so I can get the base part of that over there,
22:19 | I want to put that one there before I put this over here,
22:22 | finally I get to the point where I can move the bottom
22:24 | one over, now I've got to be really careful to make sure
22:26 | that I don't cover up the bottom one in the wrong way
22:28 | before I get to the stage where I wish they were posts
22:30 | and there you go.
22:31 | All right?
22:32 | [APPLAUSE]
22:34 | I mean, I can make money at Harvard Square doing this
22:36 | stuff, right?
22:38 | All right, you ready to do five?
22:41 | Got the solution?
22:43 | Not so easy to see.
22:45 | All right, but this is actually a great one of those
22:47 | educational moments.
22:49 | This is a great example to think recursively.
22:52 | If I wanted to think about this problem recursively--
22:54 | what do I mean by thinking recursively?
22:55 | How do I reduce this to a smaller-size problem in the
22:59 | same instant?
23:00 | And so, if I do that, this now becomes really easy.
23:02 | If I want to move this stack here, I'm going to take a
23:06 | stack of size n minus 1, move it to the spare spot, now I
23:10 | can move the base disk over, and then I'm going to move
23:13 | that stack of size n minus 1 to there.
23:18 | That's literally what I did, OK?
23:21 | So there's the code.
23:22 | Called towers.
23:23 | I'm just going to have you-- let you take a look at it.
23:26 | I'm giving it an argument, which is the size of the
23:27 | stack, and then just labels for the three posts.
23:30 | A from, a to, and a spare.
23:33 | And in fact, if we look at this-- let me just pop it over
23:36 | to the other side--
23:38 | OK, I can move a tower, I'll say of size 2, from, to, and
23:44 | spare, and that was what I did.
23:48 | And if I want to move towers, let's say, size 5, from, to,
23:55 | and spare, there are the instructions
24:01 | for how to move it.
24:02 | We ain't going to do sixty-four.
24:05 | OK.
24:06 | All right.
24:07 | So it's fun, and I got a little bit of applause out of
24:09 | it, which is always nice for me, but I also showed you how
24:13 | to think about it recursively.
24:14 | Once you hear that description, it's easy to
24:16 | write the code, in fact.
24:18 | This is a place where the recursive version of it is
24:20 | much easier to think about than the iterative one.
24:22 | But what I really want to talk about is, what's the order of
24:25 | growth here?
24:25 | What's the complexity of this algorithm?
24:28 | And again, I'm going to do it with a little bit of abusive
24:31 | notation, and it's a little more complicated, but we can
24:33 | kind of look at.
24:34 | All right?
24:35 | Given the code up there, if I want to move a tower of size
24:40 | n, what do I have to do?
24:43 | I've got to test to see if I'm in the base case, and if I'm
24:47 | not, then I need to move a tower of size n minus 1, I
24:52 | need to move a tower of size 1, and I need to move a
24:56 | second-- sorry about that-- a second tower of
24:59 | size n minus 1.
25:02 | OK. t of 1 I can also reduce.
25:03 | In the case of a tower of size 1, basically there are two
25:06 | things to do, right?
25:07 | I've got to do the test, and then I just do the move.
25:09 | So the general formula is that.
25:16 | Now.
25:18 | You might look at that and say, well that's just a lot
25:20 | like what we had over here.
25:22 | Right?
25:22 | We had some additive constant plus a simpler version of the
25:25 | same problem reduced in size by 1.
25:28 | But that two matters.
25:31 | So let's look at it.
25:32 | How do I rea-- replace the expression FOR t of n minus 1?
25:35 | Substitute it in again. t of n minus 1 is 3 plus 2
25:39 | t of n minus 2.
25:40 | So this is 3, plus 2 times 3, plus 4 t minus 2.
25:49 | OK.
25:51 | And if I substitute it again, I get 3 plus 2 times 3 plus 4
25:56 | times 3 plus 8 t n minus 3.
26:02 | This is going by a little fast. I'm just
26:04 | substituting in.
26:05 | I'm going to skip some steps.
26:08 | But basically if I do this, I end up with 3 times 1 plus 2
26:13 | plus 4 to 2 to the k minus 1 for all of
26:17 | those terms, plus 2--
26:21 | I want to do this right, 2 to the k, sorry-- t of n minus k.
26:30 | OK.
26:31 | Don't sweat the details, I'm just expanding it out.
26:33 | What I want you to see is, because I've got two versions
26:36 | of that problem.
26:38 | The next time down I've got four versions.
26:39 | Next time down I've got eight versions.
26:40 | And in fact, if I substitute, I can solve for this, I'm done
26:43 | when this is equal to 1.
26:45 | If you substitute it all in, you get basically
26:49 | order 2 to the n.
26:54 | Exponential.
26:57 | That's a problem.
26:59 | Now, it's also the case that this is fundamentally what
27:02 | class this algorithm falls into, it is going to take
27:04 | exponential amount of time.
27:06 | But it grows pretty rapidly, as n goes up, and I'm going to
27:10 | show you an example in a second.
27:12 | Again, what I want you to see is, notice the
27:14 | characteristic of that.
27:15 | That this recursive call had two sub-problems of a smaller
27:21 | size, not one.
27:22 | And that makes a big difference.
27:23 | So just to show you how big a difference it makes, let's run
27:26 | a couple of numbers.
27:29 | Let's suppose n is 1000, and we're running
27:33 | at nanosecond speed.
27:37 | We have seen log, linear, quadratic, and exponential.
27:52 | So, again, there could be constants in here, but just to
27:55 | give you a sense of this.
27:56 | If I'm running at nanosecond speed, n, the size of the
27:59 | problem, whatever it is, is 1000, and I've got a log
28:02 | algorithm, it takes 10 nanoseconds to complete.
28:08 | If you blink, you miss it.
28:12 | If I'm running a linear algorithm, it'll take one
28:18 | microsecond to complete.
28:21 | If I'm running a quadratic algorithm, it'll take one
28:25 | millisecond to complete.
28:29 | And if I'm running an exponential
28:32 | algorithm, any guesses?
28:33 | I hope Washington doesn't take this long to fix my 401k plan.
28:50 | All right?
28:51 | 10 to the 284 years.
28:54 | As Emeril would say, pow!
28:56 | That's a some spicy whatever.
28:58 | All right.
28:59 | Bad jokes aside, what's the point?
29:01 | You see, these classes have really different performance.
29:04 | Now this is a little misleading.
29:05 | These are all really fast, so just to give you another set
29:08 | of examples, I'm not going to do the--
29:10 | If I had a problem where the log one took ten milliseconds,
29:17 | then the linear one would take a second, the quadratic one
29:21 | would take 16 minutes.
29:26 | So you can see, even the quadratic ones can
29:29 | blow up in a hurry.
29:30 | And this goes back to the point I tried
29:31 | to make last time.
29:33 | Yes, the computers are really fast. But the problems can
29:36 | grow much faster than you can get a performance boost out of
29:38 | the computer.
29:39 | And you really, wherever possible, want to avoid that
29:42 | exponential algorithm, because that's really deadly.
29:44 | Yes.
29:48 | All right.
29:50 | The question is, is there a point where it'll quit.
29:52 | Yeah, when the power goes out, or-- so let me not answer it
29:56 | quite so facetiously.
29:57 | We'd be mostly talking about time.
29:58 | In fact, if I ran one of these things, it would just keep
30:00 | crunching away.
30:01 | It will probably quit at some point because of space issues,
30:05 | unless I'm writing an algorithm that is using no
30:07 | additional space.
30:08 | Right.
30:09 | Those things are going to stack up, and eventually it's
30:10 | going to run out of space.
30:11 | And that's more likely to happen, but, you know.
30:13 | The algorithm doesn't know that it's going to take this
30:17 | long to compute, it's just busy crunching away, trying to
30:19 | see if it can make it happen.
30:21 | OK.
30:21 | Good question, thank you.
30:24 | All right.
30:26 | I want to do one more extended example here., because we've
30:29 | got another piece to do, but I want to capture this, because
30:31 | it's important, so let me again try and say it the
30:33 | following way.
30:34 | I want you to recognize classes of algorithms and
30:37 | match what you see in the performance of the algorithm
30:41 | to the complexity of that algorithm.
30:42 | All right?
30:44 | Linear algorithms tend to be things where, at one
30:47 | pass-through, you reduce the problem by a
30:50 | constant amount, by one.
30:51 | If you reduce it by two, it's going to be the same thing.
30:53 | Where you go from problem of size n to a problem of
30:55 | size n minus 1.
30:57 | A log algorithm typically is one where you cut the size of
31:01 | the problem down by some multiplicative factor.
31:03 | You reduce it in half.
31:04 | You reduce it in third.
31:05 | All right?
31:07 | Quadratic algorithms tend to have this--
31:09 | I was about to say additive, wrong term-- but
31:11 | doubly-nested, triply-nested things are likely to be
31:15 | quadratic or cubic algorithms, all right, because you know--
31:18 | let me not confuse things-- double-loop quadratic
31:20 | algorithm, because you're doing one set of things and
31:23 | you're doing it some other number of times, and that's a
31:25 | typical signal that that's what you have there.
31:27 | OK.
31:28 | And then the exponentials, as you saw is when typically I
31:30 | reduce the problem of one size into two or more sub-problems
31:36 | of a smaller size.
31:37 | And you can imagine this gets complex and there's lots of
31:39 | interesting things to do to look to the real form, but
31:41 | those are the things that you should see.
31:43 | Now.
31:44 | Two other things, before we do this last example.
31:47 | One is, I'll remind you, what we're interested in is
31:50 | asymptotic growth.
31:51 | How does this thing grow as I make the problem size big?
31:55 | And I'll also remind you, and we're going to see this in the
31:56 | next example, we talked about looking at
31:58 | the worst case behavior.
32:00 | In these cases there's no best case worst case, it's just
32:02 | doing one computation.
32:03 | We're going to see an example of that in a second.
32:05 | What we really want to worry about, what's the worst case
32:07 | that happens.
32:09 | And the third thing I want you to keep in mind is, remember
32:11 | these are orders of growth.
32:14 | It is certainly possible, for example, that a quadratic
32:17 | algorithm could run faster than a linear algorithm.
32:20 | It depends on what the input is, it depends on, you know,
32:24 | what the particular cases are.
32:25 | So it is not the case that, on every input, a linear
32:28 | algorithm is always going to be better
32:29 | than a quadratic algorithm.
32:30 | It is just in general that's going to hold true, and that's
32:33 | what I want you to see.
32:35 | OK.
32:36 | I want to do one last example.
32:37 | I'm going to take a little bit more time on it, because it's
32:41 | going to both reinforce these ideas, but it's also going to
32:43 | show us how we have to think about what's a primitive
32:46 | step., and in a particular, how do data structures
32:49 | interact with this analysis?
32:51 | Here I've just been running integers, it's pretty simple,
32:53 | but if I have a data structure, I'm going to have
32:55 | to worry about that a little bit more.
32:56 | So let's look at that.
32:57 | And the example I want to look at is, suppose I want to
33:00 | search a list that I know is sorted, to see if an element's
33:04 | in the list. OK?
33:05 | So the example I'm going to do, I'm going to search a
33:17 | sorted list. All right.
33:20 | If you flip to the second side of your handout, you'll see
33:23 | that I have a piece of code there, that does this-- let
33:26 | me, ah, I didn't want to do that, let me back up
33:28 | slightly-- this is the algorithm called search.
33:32 | And let's take a look at it.
33:35 | OK?
33:36 | Basic idea, before I even look at the code, is pretty simple.
33:38 | If I've got a list that is sorted, in let's call it, just
33:41 | in increasing order, and I haven't said what's in the
33:43 | list, could be numbers, could be other things, for now,
33:45 | we're going to just assume they're integers.
33:46 | The easy thing to do would be the following: start at the
33:50 | front end of the list, check the first element.
33:53 | If it's the thing I'm looking for, I'm done.
33:54 | It's there.
33:54 | If not, move on to the next element.
33:57 | And keep doing that.
33:57 | But if, at any point, I get to a place in the list where the
34:00 | thing I'm looking for is smaller than the element in
34:04 | the list, I know everything else in the rest of the list
34:07 | has to be bigger than that, I don't have to
34:08 | bother looking anymore.
34:09 | It says the element's not there.
34:10 | I can just stop.
34:12 | OK.
34:12 | So that's what this piece of code does here.
34:14 | Right.?
34:15 | I'm going to set up a variable to say, what's the answer I
34:17 | want to return, is it there or not.
34:19 | Initially it's got that funny value none.
34:22 | I'm going to set up an index, which is going to tell me
34:25 | where to look, starting at the first part of the list, right?
34:29 | And then, when I got--
34:30 | I'm also going to count how many comparisons I do, just so
34:32 | I can see how much work I do here, and then
34:34 | notice what it does.
34:35 | It says while the index is smaller than the size of the
34:40 | list, I'm not at the end of the list, and I don't have an
34:43 | answer yet, check.
34:45 | So I'm going to check to see if-- really can't read that
34:49 | thing, let me do it this way-- right, I'm going to increase
34:51 | the number of compares, and I'm going to check to say, is
34:53 | the thing I'm looking for at the i'th spot in the list?
34:57 | Right, so s of i saying, given the list, look at the i'th
34:59 | element, is it the same thing?
35:01 | If it is, OK.
35:04 | Set the answer to true.
35:06 | Which means, next time through the loop, that's going to pop
35:09 | out and return an answer.
35:11 | If it's not, then check to see, is it smaller than that
35:17 | element in the current spot of the list?
35:19 | And if that's true, it says again, everything else in the
35:22 | list has to be bigger than this, thing can't possibly be
35:24 | in the list, I'm taking advantage of the ordering, I
35:26 | can set the answer to false, change i to go to the next
35:29 | one, and next time through the loop, I'm going to pop out and
35:31 | print it out.
35:34 | OK?
35:36 | Right.
35:37 | Order of growth here.
35:38 | What do you think?
35:46 | Even with these glasses on, I can see no hands up, any
35:48 | suggestions?
35:49 | Somebody help me out.
35:50 | What do you think the order of growth is here?
35:51 | I've got a list, walk you through it an element at a
35:58 | time, do I look at each element of the
36:00 | list more than once?
36:02 | Don't think so, right?
36:04 | So, what does this suggest?
36:07 | Sorry?
36:09 | Constant.
36:10 | Ooh, constant says, no matter what the length of the list
36:12 | is, I'm going to take the same amount of time.
36:16 | And I don't think that's true, right?
36:17 | If I have a list ten times longer, it's going to take me
36:21 | more time, so-- not a bad guess, I'm still
36:23 | reward you, thank you.
36:27 | Somebody else.
36:29 | Yeah.
36:30 | Linear.
36:30 | Why?
36:31 | You're right, by the way, but why?
36:39 | Yeah.
36:40 | All right, so the answer was it's linear, which is
36:41 | absolutely right.
36:43 | Although for a reason we're going to
36:44 | come back in a second.
36:46 | Oh, thank you, I hope your friends help you out with
36:48 | that, thank you.
36:49 | Right?
36:50 | You can see that this ought to be linear,
36:52 | because what am I doing?
36:53 | I'm walking down the list. So one of the things I didn't
36:56 | say, it's sort of implicit here, is what is the thing I
36:58 | measuring the size of the problem in?
36:59 | What's the size of the list?
37:01 | And if I'm walking down the list, this is probably order
37:08 | of the length of the list s, because I'm looking at each
37:10 | element once.
37:13 | Now you might say, wait a minute.
37:14 | Thing's ordered, if I stop part way through and I throw
37:17 | away half the list, doesn't that help me?
37:19 | And the answer is yes, but it doesn't change the complexity.
37:22 | Because what did we say?
37:23 | We're measuring the worst case.
37:26 | The worst case here is, the things not in the list, in
37:28 | which case I've got to go all the way through the list to
37:30 | get to the end.
37:32 | OK.
37:33 | Now, having said that, and I've actually got a subtlety
37:37 | I'm going to come back to in a second, there ought to be a
37:39 | better way to do this.
37:40 | OK?
37:41 | And here's the better way to think about.
37:47 | I'll just draw out sort of a funny representation of a
37:49 | list. These are sort of the cells, if you like, in memory
37:55 | that are holding the elements of the list. What we've been
37:58 | saying is, I start here and look.
38:00 | If it's there, I'm done.
38:00 | If not, I go there.
38:02 | If it's there, I'm done, if not, I keep walking down, and
38:04 | I only stop when I get to a place where the element I'm
38:07 | looking for is smaller than the value in the list., in
38:10 | which case I know the rest of this is too
38:11 | big and I can stop.
38:13 | But I still have to go through the list.
38:16 | There's a better way to think about this, and in fact
38:17 | Professor Guttag has already hinted at this in the last
38:20 | couple of lectures.
38:22 | The better way to think about this is, suppose, rather than
38:24 | starting at the beginning, I just grabbed some spot at
38:26 | random, like this one.
38:29 | And I look at that value.
38:32 | If it's the value I'm looking for, boy, I ought to go to
38:34 | Vegas, I'm really lucky.
38:35 | And I'm done, right?
38:37 | If not, what could I do?
38:39 | Well, I could look at the value here, and compare it to
38:41 | the value I'm trying to find, and say the following; if the
38:45 | value I'm looking for is bigger than this value, where
38:51 | do I need to look?
38:53 | Just here.
38:53 | All right?
38:57 | Can't possibly be there, because I know
38:59 | this thing is over.
39:01 | On the other hand, if the value I'm looking for here--
39:03 | sorry, the value I'm looking for is smaller than the value
39:06 | I see here, I just need to look here.
39:10 | All right?
39:14 | Having done that, I could do the same thing, so I suppose I
39:17 | take this branch, I can pick a spot like, say, this one, and
39:20 | look there.
39:21 | Because there, I'm done, if not, I'm either
39:23 | looking here or there.
39:27 | And I keep cutting the problem down.
39:31 | OK.
39:31 | Now, having said that, where should I pick to
39:35 | look in this list?
39:39 | I'm sorry?
39:40 | Halfway.
39:41 | Why?
39:41 | You're right, but why?
39:50 | Yeah.
39:50 | So the answer, in case you didn't hear it, was, again, if
39:53 | I'm a gambling person, I could start like a way down here.
39:55 | All right?
39:57 | If I'm gambling, I'm saying, gee, if I'm really lucky,
39:59 | it'll be only on this side, and I've got a little bit of
40:01 | work to do, but if I'm unlucky, I'm scrawed, the past
40:03 | pluperfect of screwed, OK., or a Boston fish.
40:07 | I'll look at the rest of that big chunk of the list, and
40:10 | that's a pain.
40:11 | So halfway is the right thing to do, because at each step,
40:15 | I'm guaranteed to throw away at least half the list. Right?
40:18 | And that's nice.
40:20 | OK.
40:21 | What would you guess the order of growth here is?
40:25 | Yeah.
40:26 | Why?
40:29 | Good.
40:29 | Exactly.
40:30 | Right?
40:30 | Again, if you didn't hear it, the answer was it's log.
40:33 | Because I'm cutting down the problem in half at each time.
40:36 | You're right, but there's something we have to do to add
40:38 | to that, and that's the last thing I want to pick up on.
40:40 | OK.
40:41 | Let's look at the code-- actually, let's test this out
40:43 | first before we do it.
40:45 | So I've added, as Professor Guttag did-- ah, should have
40:47 | said it this way, let's write the code for it first, sorry
40:51 | about that--
40:52 | OK, I'm going to write a little thing called b search.
40:54 | I'm going to call it down here with search, which is simply
40:57 | going to call it, and then print an answer out.
41:00 | In binary search-- ah, there's that wonderful phrase, this is
41:03 | called a version of binary search, just like you saw
41:05 | bin-- or bi-section methods, when we were doing numerical
41:07 | things-- in binary search, I need to keep track of the
41:11 | starting point and the ending point of the
41:13 | list I'm looking at.
41:14 | Initially, it's the beginning and the end of it.
41:16 | And when I do this test, what I want to do, is say I'm going
41:18 | to pick the middle spot, and depending on the test, if I
41:21 | know it's in the upper half, I'm going to set my start at
41:24 | the mid point and the end stays the same, if it's in the
41:27 | front half I'm going to keep the front the same and I'm
41:29 | going to change the endpoint.
41:30 | And you can see that in this code here.
41:33 | Right?
41:34 | What does it say to do?
41:34 | It says, well I'm going to print out first and last, just
41:37 | so you can see it, and then I say, gee, if last minus first
41:41 | is less than 2, that is, if there's no more than two
41:43 | elements left in the list, then I can just check those
41:46 | two elements, and return the answer.
41:50 | Otherwise, we find the midpoint, and
41:52 | notice what it does.
41:53 | First, it's pointing to the beginning of the list, which
41:55 | initially might be down here at 0 but after a while, might
41:58 | be part way through.
41:59 | And to that, I simply add a halfway
42:03 | point, and then I check.
42:06 | If it's at that point, I'm done, if not, if it's greater
42:08 | than the value I'm looking for, I either take
42:11 | one half or the other.
42:15 | OK.
42:15 | You can see that thing is cutting down the problem in
42:17 | half each time, which is good, but there's one more thing I
42:19 | need to deal with.
42:20 | So let's step through this with a little more care.
42:22 | And I keep saying, before we do it, let's just
42:23 | actually try it out.
42:24 | So I'm going to go over here, and I'm going
42:27 | to type test search--
42:29 | I can type-- and if you look at your handout, it's just a
42:33 | sequence of tests that I'm going to do.
42:36 | OK.
42:36 | So initially, I'm going to set up the list to be the first
42:39 | million integers.
42:40 | Yeah, it's kind of simple, but it gives me an ordered list of
42:43 | these things, And let's run it.
42:44 | OK.
42:46 | So I'm first going to look for something that's not in the
42:48 | list, I'm going to see, is minus 1 in this list, so it's
42:50 | going to be at the far end, and if I do that in
42:52 | the basic case, bam.
42:54 | Done.
42:54 | All right?
42:56 | The basic, that primary search, because it looks at
42:57 | the first element, says it's smaller than
42:59 | everything else, I'm done.
43:00 | If I look in the binary case, takes a little longer.
43:06 | Notice the printout here.
43:07 | The printout is simply telling me, what are the
43:10 | ranges of the search.
43:11 | And you can see it wrapping its way down, cutting in half
43:15 | at each time until it gets there, but it
43:16 | takes a while to find.
43:18 | All right.
43:18 | Let's search to see though now if a million is in this list,
43:23 | or 10 million, whichever way I did this, it must be a
43:25 | million, right?
43:26 | In the basic case, oh, took a little while.
43:31 | Right, in the binary case, bam.
43:35 | In fact, it took the same number of steps as it did in
43:37 | the other case, because each time I'm cutting
43:39 | it down by a half.
43:40 | OK.
43:41 | That's nice.
43:42 | Now, let's do the following; if you look right here, I'm
43:43 | going to set this now to--
43:45 | I'm going to change my range to 10 million, I'm going to
43:48 | first say, gee, is a million in there,
43:51 | using the basic search.
43:54 | It is.
43:56 | Now, I'm going to say, is 10 million in this, using the
43:58 | basic search.
44:01 | We may test your hypothesis, about how long does it take,
44:04 | if I time this really well, I ought to be able to end when
44:06 | it finds it, which should be right about now.
44:10 | That was pure luck.
44:13 | But notice how much longer it took.
44:14 | On the other hand, watch what happens with binary.
44:16 | Is the partway one there?
44:18 | Yeah.
44:19 | Is the last one there?
44:21 | Wow.
44:22 | I think it took one more step.
44:25 | Man, that's exactly what logs should do, right?
44:27 | I make the problem ten times bigger, it takes one
44:30 | more step to do it.
44:31 | Whereas in the linear case, I make it ten times bigger, it
44:34 | takes ten times longer to run.
44:36 | OK.
44:38 | So I keep saying I've got one thing hanging, it's the last
44:40 | thing I want to do, but I wanted you see how much of a
44:42 | difference this makes.
44:43 | But let's look a little more carefully at the code for
44:46 | binary search-- for search 1.
44:48 | What's the complexity of search 1?
44:50 | Well, you might say it's constant, right?
44:52 | It's only got two things to do, except what it really says
44:54 | is, that the complexity of search 1 is the same as the
44:56 | complexity of b search, because that's
44:58 | the call it's doing.
44:59 | So let's look at b search.
45:00 | All right?
45:02 | We've got the code for b search up there.
45:05 | First step, constant, right?
45:08 | Nothing to do.
45:09 | Second step, hm.
45:13 | That also looks constant, you think?
45:17 | Oh but wait a minute.
45:19 | I'm accessing s.
45:21 | I'm accessing a list. How long does it take for me to get the
45:25 | nth element of a list?
45:28 | That might not be a primitive step.
45:31 | And in fact, it depends on how I store a list.
45:35 | So, for example, in this case, I had lists that I knew were
45:42 | made out of integers.
45:44 | As a consequence, I have a list of ints.
45:50 | I might know, for example, that it takes four memory
45:53 | chunks to represent one int, just for example.
45:56 | And to find the i'th element, I'm simply going to take the
46:01 | starting point, that point at the beginning of memory where
46:04 | the list is, plus 4 times i, that would tell me how many
46:10 | units over to go, and that's the memory location I want to
46:13 | look for the i'th element of the list.
46:15 | And remember, we said we're going to assume a random
46:17 | access model, which says, as long as I know the location,
46:21 | it takes a constant amount of time to get to that point.
46:24 | So if the-- if I knew the lists were made of just
46:27 | integers, it'd be really easy to figure it out.
46:30 | Another way of saying it is, this takes constant amount of
46:32 | time to figure out where to look, it takes constant amount
46:34 | of time to get there, so in fact I could treat indexing
46:37 | into a list as being a basic operation.
46:41 | But we know lists can be composed of anything.
46:44 | Could be ints, could be floats, could be a combination
46:47 | of things, some ints, some floats, some lists, some
46:49 | strings, some lists of lists, whatever.
46:51 | And in that case, in general lists, I need to figure out
46:58 | what's the access time.
47:01 | And here I've got a choice.
47:03 | OK, one of the ways I could do would be the following.
47:05 | I could have a pointer to the beginning of the list where
47:10 | the first element here is the actual value, and this would
47:17 | point to the next element in the list. Or another way of
47:23 | saying it is, the first part of the cell could be some
47:26 | encoding of how many cells do I need to have to store the
47:29 | value, and then I've got some way of telling me where to get
47:31 | the next element of the list. And this would point to value,
47:35 | and this would point off someplace in memory.
47:42 | Here's the problem with that technique, and by the way, a
47:44 | number of programming languages use
47:45 | this, including Lisp.
47:46 | The problem with that technique, while it's very
47:48 | general, is how long does it take me to find the i'th
47:51 | element of the list?
47:54 | Oh fudge knuckle.
47:55 | OK.
47:56 | I've got to go to the first place, figure out how far over
47:58 | to skip, go to the next place, figure out how far over to
48:01 | skip, eventually I'll be out the door.
48:02 | I've got to count my way down, which means that the access
48:05 | would be linear in the length of the list to find the i'th
48:09 | element of the list, and that's going to increase the
48:11 | complexity.
48:13 | There's an alternative, which is the last point I want to
48:15 | make, which is instead what I could do, I should have said
48:22 | these things are called linked lists, we'll come back to
48:25 | those, another way to do it, is to have the start of the
48:29 | list be at some point in memory, and to have each one
48:36 | of the successive cells in memory point off
48:39 | to the actual value.
48:40 | Which may take up some arbitrary amount of memory.
48:43 | In that case, I'm back to this problem.
48:47 | And as a consequence, access time in the list is constant,
48:51 | which is what I want.
48:56 | Now, to my knowledge, most implementations of Python use
48:59 | this way of storing lists, whereas Lisp
49:01 | and Scheme do not.
49:02 | The message I'm trying to get to here, because I'm running
49:05 | you right up against time, is I have to be careful about
49:09 | what's a primitive step.
49:11 | With this, if I can assume that accessing the i'th
49:14 | element of a list is constant, then you can't see that the
49:17 | rest of that analysis looks just like the log analysis I
49:20 | did before, and each step, no matter which branch I'm
49:23 | taking, I'm cutting the problem down in half.
49:25 | And as a consequence, it is log.
49:27 | And the last piece of this, is as said, I have to make sure I
49:35 | know what my primitive elements are, in terms of
49:39 | operations.
49:41 | Summary: I want you to recognize different classes of
49:44 | algorithms. I'm not going to repeat them.
49:46 | We've seen log, we've seen linear, we've seen quadratic,
49:48 | we've seen exponential.
49:50 | One of the things you should begin to do, is to recognize
49:53 | what identifies those classes of algorithms, so you can map
49:57 | your problems into those ranges.
49:59 | And with that, good luck on Thursday.
