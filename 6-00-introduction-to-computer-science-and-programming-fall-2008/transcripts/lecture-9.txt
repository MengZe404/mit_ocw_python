0:00 | ANNOUNCER: Open content is provided under a creative
0:02 | commons license.
0:03 | Your support will help MIT OpenCourseWare continue to
0:06 | offer High-quality educational resources for free.
0:10 | To make a donation, or view additional materials from
0:13 | hundreds of MIT courses, visit MIT OpenCourseWare at
0:17 | ocw.mit.edu .
0:19 | PROFESSOR ERIC GRIMSON: Let's recap where we were.
0:23 | Last lecture, we talked about, or started to talk about,
0:26 | efficiency.
0:27 | Orders of growth.
0:28 | Complexity.
0:30 | And I'll remind you, we saw a set of algorithms, and part of
0:33 | my goal was to get you to begin to recognize
0:35 | characteristics of algorithms that map into
0:38 | a particular class.
0:40 | So what did we see?
0:40 | We saw linear algorithms. Typical characterization, not
0:44 | all the time, but typical characterization, is an
0:47 | algorithm that reduces the size of a problem by one, or
0:51 | by some constant amount each time, is typically an example
0:55 | of a linear algorithm.
0:57 | And we saw a couple of examples of linear algorithms.
1:00 | We also saw a logarithmic algorithm. and we like log
1:03 | algorithms, because they're really fast. A typical
1:06 | characteristic of a log algorithm is a pro-- or sorry,
1:09 | an algorithm where it reduces the size of the problem by a
1:13 | constant factor.
1:14 | Obviously-- and that's a bad way of saying it, I said
1:16 | constant the previous time-- in the linear case, it's
1:18 | subtract by certain amount.
1:19 | In the log case, it's divide by an amount.
1:21 | Cut the problem in half.
1:23 | Cut the problem in half again.
1:25 | And that's a typical
1:26 | characterization of a log algorithm.
1:28 | We saw some quadratic algorithms, typically those
1:31 | are things with multiple nested loops, or iterative or
1:33 | recursive calls, where you're doing, say, a linear amount of
1:36 | time but you're doing it a linear number of times, and so
1:39 | it becomes quadratic, and you'll see other polynomial
1:42 | kinds of algorithms.
1:43 | And finally, we saw an example of an exponential algorithm,
1:46 | those Towers of Hanoi.
1:48 | We don't like exponential algorithms, or at least you
1:51 | shouldn't like them, because they blow up quickly.
1:53 | And we saw some examples of that.
1:55 | And unfortunately, some problems are inherently
1:58 | exponential, you're sort of stuck with that, and then you
2:00 | just have to try be as clever as you can.
2:03 | OK.
2:04 | At the end of the lecture last time, I also showed you an
2:07 | example of binary search.
2:09 | And I want to redo that in a little more detail today,
2:12 | because I felt like I did that a little more quickly than I
2:15 | wanted to, so, if you really got binary search, fall asleep
2:18 | for about ten minutes, just don't snore, your neighbors
2:21 | may not appreciate it, but we're going to go over it
2:22 | again, because it's a problem and an idea that we're going
2:24 | to come back to, and I really want to make sure that I do
2:26 | this in a way that makes real good sense you.
2:30 | Again.
2:31 | Basic premise of binary search, or at least we set it
2:33 | up was, imagine I have a sorted list of elements.
2:37 | We get, in a second, to how we're going to get them
2:39 | sorted, and I want to know, is a particular
2:42 | element in that list..
2:44 | And the basic idea of binary search is to start with the
2:47 | full range of the list, pick the midpoint,
2:50 | and test that point.
2:53 | If it's the thing I'm looking for, I'm golden.
2:54 | If not, because the list is sorted, I can use the
2:58 | difference between what I'm looking for and that midpoint
3:01 | to decide, should I look in the top half of the list, or
3:04 | the bottom half of the list?
3:05 | And I keep chopping it down.
3:06 | And I want to show you a little bit more detail of
3:09 | that, so let's create a simple little list here.
3:11 | All right?
3:22 | I don't care what's in there, but just assume that's my
3:24 | list. And just to remind you, on your handout, and there it
3:28 | is on the screen, I'm going to bring it back up, there's the
3:30 | little binary search algorithm.
3:32 | We're going to call search, which just
3:33 | calls binary search.
3:34 | And you can look at it, and let's in fact take a look at
3:39 | it to see what it does.
3:40 | We're going to call binary search, it's going to take the
3:42 | list to search and the element, but it's also going
3:44 | to say, here's the first part of the list, and there's the
3:52 | last part of the list, and what does it
3:54 | do inside that code?
3:55 | Well, it checks to see, is it bigger than two?
3:57 | Are there more than two elements there?
3:59 | If there are less than two elements there, I just check
4:01 | one or both of those to see if I'm looking
4:03 | for the right thing.
4:04 | Otherwise, what does that code say to do?
4:06 | It says find the midpoint, which says, take the start,
4:10 | which is pointing to that place right there, take last
4:15 | minus first, divide it by 2, and add it to start.
4:17 | And that basically, somewhere about here,
4:21 | gives me the midpoint.
4:23 | Now I look at that element.
4:25 | Is it the thing I'm looking for?
4:26 | If I'm really lucky, it is.
4:29 | If not, I look at the value of that point here and the thing
4:33 | I'm looking for.
4:34 | And for sake of argument, let's assume that the thing
4:36 | I'm looking for is smaller than the value here.
4:39 | Here's what I do.
4:41 | I change-- oops!
4:42 | Let me do that this way--
4:43 | I change last to here, and keep first there, and I throw
4:51 | away all of that.
4:54 | All right?
4:56 | That's just the those-- let me use my pointer-- that's just
4:59 | these two lines here.
5:01 | I checked the value, and in one case, I'm changing the
5:05 | last to be mid minus 1, which is the case I'm in here, and I
5:09 | just call again.
5:10 | All right?
5:12 | I'm going to call exactly the same thing.
5:13 | Now, first is pointing here, last is pointing there, again,
5:16 | I check to see, are there more than two things left?
5:18 | There are, in this case.
5:20 | So what do I do?
5:21 | I find the midpoint by taking last minus first, divide by 2,
5:24 | and add to start.
5:25 | Just for sake of argument, we'll assume it's about there,
5:29 | and I do the same thing.
5:31 | Is this value what I'm looking for?
5:33 | Again, for sake of argument, let's assume it's not.
5:35 | Let's assume, for sake of argument, the thing I'm
5:38 | looking for is bigger than this.
5:40 | In that case, I'm going to throw away all of this, I'm
5:43 | going to hit that bottom line of that code.
5:46 | Ah.
5:47 | What does that do?
5:48 | It changes the call.
5:49 | So in this case, first now points
5:54 | there, last points there.
6:00 | And I cut around.
6:01 | And again, notice what I've done.
6:07 | I've thrown away most of the array-- most of the list, I
6:09 | shouldn't say array-- most of the list. All right?
6:12 | So it cuts it down quickly as we go along.
6:16 | OK.
6:18 | That's the basic idea of binary search.
6:20 | And let's just run a couple of examples to remind you of what
6:23 | happens if we do this.
6:27 | So if I call, let's [UNINTELLIGIBLE], let's set up
6:29 | s to be, I don't know, some big long list. OK.
6:37 | And I'm going to look to see, is a particular element inside
6:41 | of that list, and again, I'll remind you, that's just giving
6:47 | me the integers from zero up to 9999 something or other.
6:50 | If I look for, say, minus 1, you might go, gee, wait a
6:56 | minute, if I was just doing linear search, I would've
6:58 | known right away that minus one wasn't in this list,
7:01 | because it's sorted and it's smaller
7:02 | than the first elements.
7:03 | So this looks like it's doing a little bit of extra work,
7:06 | but you can see, if you look at that, how it cuts it down
7:09 | at each stage.
7:09 | And I'll remind you, what I'm printing out there is, first
7:12 | and last, with the range I'm looking over, and then just
7:16 | how many times the iteration called.
7:20 | So in this case, it just keeps chopping down from the back
7:22 | end, which kind of makes sense, all right?
7:24 | But in a fixed number, in fact, twenty-three calls, it
7:28 | gets down to the point of being able to say
7:29 | whether it's there.
7:30 | Let's go the other direction.
7:33 | And yes, I guess I'd better say s not 2, or we're going to
7:39 | get an error here.
7:40 | Again, in twenty-three checks.
7:48 | In this case, it's cutting up from the bottom end, which
7:50 | makes sense because the thing I'm looking for is always
7:52 | bigger than the midpoint, and then, I don't know, let's pick
7:55 | something in between.
7:58 | Somebody want-- ah, I keep doing that-- somebody like to
8:03 | give me a number?
8:05 | I know you'd like to give me other things, other
8:07 | expression, somebody give me a number.
8:10 | Anybody?
8:11 | No?
8:12 | Sorry.
8:13 | Thank you.
8:14 | Good number.
8:15 | OK, walks in very quickly.
8:23 | OK?
8:24 | And if you just look at the numbers, you can see how it
8:26 | cuts in from one side and then the other side as it keeps
8:29 | narrowing that range, until it gets down to the place where
8:31 | there are at most two things left, and then it just has to
8:34 | check those two to say whether it's there or not.
8:37 | Think about this compared to a linear search.
8:39 | All right?
8:39 | A linear search, I start at the beginning of the list and
8:41 | walk all the way through it.
8:42 | All right, if I'm lucky and it's at the low end, I'll find
8:45 | it pretty quickly.
8:46 | If it's not, if it's at the far end, I've got to go
8:48 | forever, and you saw that last time where this thing paused
8:51 | for a little while while it actually
8:52 | searched a list this big.
8:55 | OK.
8:55 | So, what do I want you to take away from this?
8:58 | This idea of binary search is going to be a
9:00 | really powerful tool.
9:02 | And it has this property, again, of
9:04 | chopping things into pieces.
9:06 | So in fact, what does that suggest about the order of
9:09 | growth here?
9:09 | What is the complexity of this?
9:12 | Yeah.
9:14 | Logarithmic.
9:14 | Why?
9:15 | STUDENT: [UNINTELLIGIBLE]
9:17 | PROFESSOR ERIC GRIMSON: Yeah.
9:18 | Thank you.
9:18 | I mean, I know I sort of said it to you, but you're right.
9:20 | It's logarithmic, right?
9:21 | It's got that property of, it cuts things in half.
9:24 | Here's another way to think about why is this log.
9:26 | Actually, let me ask a slightly different question.
9:28 | How do we know this always stops?
9:30 | I mean, I ran three trials here, and it did.
9:33 | But how would I reason about, does this always stop?
9:36 | Well let's see.
9:37 | Where's the end test on this thing?
9:40 | The end test-- and I've got the wrong glasses on-- but
9:43 | it's up here, where I'm looking to see, is last minus
9:46 | first less than or equal to 2?
9:49 | OK.
9:49 | So, soon as I get down to a list that has no more than two
9:52 | elements in it, I'm done.
9:55 | Notice that.
9:55 | It's a less than or equal to.
9:57 | What if I just tested to see if it was only, say, one?
10:00 | There was one element in there.
10:01 | Would that have worked?
10:07 | I think it depends on whether the list is
10:09 | odd or even in length.
10:11 | Actually, that's probably not true.
10:12 | With one, it'll probably always get it down there, but
10:14 | if I've made it just equal to two, I might have lost.
10:17 | So first of all, I've got to be careful about the end test.
10:19 | But the second thing is, OK, if it stops whenever this is
10:22 | less than two, am I convinced that this will always halt?
10:26 | And the answer is sure.
10:26 | Because what do I do?
10:27 | At each stage, no matter which branch, here or here, I take,
10:32 | I'm cutting down the length of the list that I'm
10:35 | searching in half.
10:36 | All right?
10:38 | So if I start off with a list of length n, how many times
10:41 | can I divide it by 2, until I get to something no
10:43 | more than two left?
10:45 | Log times, right.?
10:46 | Exactly as the gentleman said.
10:47 | Oh, I'm sorry.
10:48 | You're patiently waiting for me to reward.
10:50 | Or actually, maybe you're not.
10:53 | Thank you.
10:55 | OK.
10:56 | So this is, in fact, log.
11:08 | Now, having said that, I actually snuck
11:10 | something by you.
11:12 | And I want to spend a couple of minutes
11:14 | again reinforcing that.
11:16 | So if we look at that code, and we were little more
11:19 | careful about this, what did we say to do?
11:21 | We said look an-- sorry.
11:22 | Count the number of primitive operations in each step.
11:26 | OK.
11:26 | So if I look at this code, first of all I'm calling
11:30 | search, it just has one call, so looks like search is
11:35 | constant, except I don't know what
11:37 | happens inside of b search.
11:38 | So I've got to look at b search.
11:39 | So let's see.
11:39 | The first line, that print thing, is
11:42 | obviously constant, right?
11:44 | Just take it as a constant amount of operations But.
11:47 | let's look at the next one here, or is that second line?
11:50 | OK.
11:51 | If last minus first is greater than or equal to 2-- sorry,
11:54 | less than 2, then either look at this thing or
11:57 | look at that thing.
11:58 | And that's where I said we've got to be careful.
12:02 | That's accessing an element of a list. We have to make sure
12:05 | that, in fact, that operation is not linear.
12:08 | So let me expand on that very slightly, and again, we did
12:12 | this last time but I want to do one more time.
12:14 | I have to be careful about how I'm actually implementing a
12:22 | list.
12:22 | So, for example: in this case, my list
12:38 | is a bunch of integers.
12:40 | And one of the things I could take advantage of, is I'm only
12:42 | going to need a finite amount of space to
12:44 | represent an integer.
12:46 | So, for example, if I want to allow for some fairly large
12:49 | range of integers, I might say, I need four memory cells
12:52 | in a row to represent an integer.
12:54 | All right, if it's a zero, it's going to be a whole bunch
12:56 | of ones-- of zeroes, so one, it may be a whole bunch of
12:59 | zeroes in the first three and then a one at the end of this
13:01 | thing, but one of the way to think about this list in
13:04 | memory, is that I can decide in constant time how to find
13:08 | the i'th element of a list.
13:09 | So in particular, here's where the zero-th element of the
13:12 | list starts, there's where the first element starts, here's
13:15 | where the third element starts, these are just memory
13:17 | cells in a row, and to find the zero-th element, if start
13:25 | is pointing to that memory cell, it's just at start.
13:29 | To find the first element, because I know I need four
13:33 | memory cells to represent an integer, it's at start plus 4.
13:41 | To get to the second element, I know that that's-- you get
13:46 | the idea-- at the start plus 2 times 4, and to get to the
13:50 | k'th element, I know that I want to take whatever the
14:01 | start is which points to that place in memory, take care,
14:04 | multiply by 4, and that tells me exactly where to go to find
14:08 | that location.
14:09 | This may sound like a nuance, but it's important.
14:13 | Why?
14:14 | Because that's a constant access, right?
14:16 | To get any location in memory, to get to any value of the
14:19 | list, I simply have to say which element do I want to
14:23 | get, I know that these things are stored in a particular
14:25 | size, multiply that index by 4, add it to start, and then
14:29 | it's in a constant amount of time I can go to that location
14:31 | and get out the cell.
14:33 | OK.
14:36 | That works nicely if I know that I have things stored in
14:42 | constant size.
14:44 | But what if I have a list of lists?
14:47 | What if I have a homogeneous list, a list of integers and
14:49 | strings and floats and lists and lists of lists and lists
14:52 | of lists of lists and all that sort of cool stuff?
14:53 | In that case, I've got to be a lot more careful.
15:04 | So in this case, one of the standard ways to do this, is
15:07 | to use what's called a linked list. And I'm going to do it
15:13 | in the following way.
15:14 | Start again, we'll point to the beginning of the list. But
15:21 | now, because my elements are going to take different
15:23 | amounts of memory, I'm going to do the following thing.
15:26 | In the first spot, I'm going to store something that says,
15:31 | here's how far you have to jump to
15:33 | get to the next element.
15:35 | And then, I'm going to use the next sequence of things to
15:38 | represent the first element, or
15:40 | the zero-th element, if you like.
15:42 | In this case I might need five.
15:44 | And then in the next spot, I'm going to say how far you have
15:47 | to jump to get to the next element.
15:49 | All right, followed by whatever I need to represent
15:53 | it, which might only be a blank one.
15:54 | And in the next spot, maybe I've got a really long list,
15:59 | and I'm going to say how to jump to
16:00 | get to the next element.
16:02 | All right, this is actually kind of nice.
16:05 | This lets me have a way of representing things that could
16:07 | be arbitrary in size.
16:08 | And some of these things could be huge, if
16:10 | they're themselves lists.
16:12 | Here's the problem.
16:13 | How do I get to the nth-- er, the k'th element in the list,
16:16 | in this case?
16:17 | Well I have to go to the zero-th element, and say OK,
16:21 | gee, to get to the next element, I've got
16:23 | to jump this here.
16:24 | And to get to the next element, I've got to jump to
16:27 | here, and to get to the next element, I've got to jump to
16:29 | here, until I get there.
16:32 | And so, I get some power.
16:34 | I get the ability to store arbitrary things, but what
16:37 | just happened to my complexity?
16:39 | How long does it take me to find the
16:41 | k'th element?
16:43 | Linear.
16:44 | Because I've got to walk my way down it.
16:45 | OK?
16:46 | So in this case, you have linear access.
16:56 | Oh fudge knuckle.
16:57 | Right?
16:58 | If that was the case in that code, then my complexity is no
17:01 | longer log, because I need linear access for each time
17:04 | I've got to go to the list, and it's going to be much
17:06 | worse than that.
17:06 | All right.
17:08 | Now.
17:08 | Some programming languages, primarily Lisp, actually store
17:13 | lists these ways.
17:15 | You might say, why?
17:17 | Well it turns out there's some trade-offs to it.
17:19 | It has some advantages in terms of power of storing
17:22 | things, it has some disadvantages, primarily in
17:24 | terms of access time.
17:26 | Fortunately for you, Python decided, or the investors of
17:28 | Python decided, to store this a different way.
17:30 | And the different way is to say, look, if I redraw this,
17:34 | it's called a box and pointer diagram, what we really have
17:48 | for each element is two things.
17:49 | And I've actually just reversed the order here.
17:52 | We have a pointer to the location in memory that
17:55 | contains the actual value, which itself might be a bunch
17:58 | of pointers, and we have a pointer to the actual-- sorry,
18:02 | a pointer the value and we have a pointer to the next
18:05 | element in the list. All right?
18:08 | And one of the things we could do if we look at that is, we
18:10 | say, gee, we could reorganize this in a pretty
18:12 | straightforward way.
18:13 | In particular, why don't we just take all of the first
18:21 | cells and stick them together?
18:35 | Where now, my list is a list of pointers, it's not a set of
18:40 | values but it's actually a pointer off to some other
18:42 | piece of memory that contains the value.
18:44 | Why is this nice?
18:46 | Well this is exactly like this.
18:50 | All right?
18:53 | It's now something that I can search in constant time.
18:57 | And that's what's going to allow me to keep this
18:58 | thing as being log.
19:01 | OK.
19:03 | With that in mind, let's go back to where we were.
19:07 | And where were we?
19:10 | We started off talking about binary search, and I suggested
19:15 | that this was a log algorithm, which it is, which is really
19:18 | kind of nice.
19:21 | Let's pull together what this algorithm actually does.
19:32 | If I generalize binary search, here's what I'm going to stake
19:35 | that this thing does.
19:37 | It says one: pick the midpoint.
19:45 | Two: check to see if this is the answer, if this is the
19:56 | thing I'm looking for.
19:58 | And then, three: if not, reduce to a smaller problem,
20:05 | and repeat.
20:16 | OK, you're going, yeah, come on, that makes obvious sense.
20:18 | And it does.
20:18 | But I want you to keep that template in mind, because
20:21 | we're going to come back to that.
20:22 | It's an example of a very common tool that's going to be
20:25 | really useful to us, not just for doing search, but for
20:28 | doing a whole range of problems. That is, in essence,
20:30 | the template the describes a log style algorithm.
20:34 | And we're going to come back to it.
20:37 | OK.
20:38 | With that in mind though, didn't I cheat?
20:41 | I remind you, I know you're not really listening to me,
20:45 | but that's OK.
20:45 | I reminded you at the beginning of the lecture, I
20:47 | said, let's assume we have a sorted list, and then
20:50 | let's go search it.
20:52 | Where in the world
20:52 | did that sorted list come from?
20:54 | What if I just get a list of elements, what do I do?
20:58 | Well let's see.
20:59 | My fall back is, I could just do linear search, walk down
21:02 | the list one at a time, just comparing those things.
21:04 | OK.
21:04 | So that's sort of my base.
21:06 | But what if I wanted, you know, how do I want to get to
21:08 | that sorted list?
21:09 | All right?
21:12 | Now.
21:16 | One of the questions, before we get to doing the sorting,
21:18 | is even to ask, what should I do in a search case like that?
21:20 | All right, so in particular, does it make sense, if I'm
21:26 | given an unsorted list, to first sort it,
21:29 | and then search it?
21:31 | Or should I just use the basically linear case?
21:34 | All right?
21:34 | So, here's the question.
21:39 | Should we sort before we search?
21:47 | OK.
21:47 | So let's see, if I'm going to do this, how fast could we
21:51 | sort a list?
21:53 | Can we sort a list in sublinear time?
22:05 | Sublinear meaning, something like log
22:07 | less than linear time?
22:08 | What do you think?
22:11 | It's possible?
22:17 | Any thoughts?
22:19 | Don't you hate professors who stand here waiting for you to
22:22 | answer, even when they have candy?
22:25 | Does it make sense to think we could do this in less than
22:28 | linear time?
22:28 | You know, it takes a little bit of thinking.
22:31 | What would it mean--
22:31 | [UNINTELLIGIBLE PHRASE] do I see a hand, way at the back,
22:34 | yes please?
22:37 | Thank you.
22:39 | Man, you're going to really make me work here, I have no
22:41 | idea if I can get it that far, ah, your friend
22:43 | will help you out.
22:44 | Thank you.
22:45 | The gentleman has it exactly right.
22:47 | How could I possibly do it in sublinear time, I've got to
22:49 | look at least every element once.
22:52 | And that's the kind of instinct I'd like you to get
22:54 | into thinking about.
22:55 | So the answer here is no.
22:58 | OK.
23:00 | Can we sort it in linear time?
23:07 | Hmmm.
23:07 | That one's not so obvious.
23:11 | So let's think about this for a second.
23:13 | To sort a list in linear time, would say, I have to look at
23:18 | each element in the list at most a
23:20 | constant number of times.
23:21 | It doesn't have to be just once, right?
23:23 | It could be two or three times.
23:25 | Hmm.
23:25 | Well, wait a minute.
23:26 | If I want to sort a list, I'll take one element, I've got to
23:28 | look at probably a lot of the other elements in the list in
23:33 | order to decide where it goes.
23:35 | And that suggests it's going to depend on how
23:37 | long the list is.
23:38 | All right, so that's a weak argument, but in fact, it's a
23:41 | way of suggesting, probably not.
23:49 | All right.
23:50 | So how fast could I sort a list?
23:52 | How fast can we sort it?
23:54 | And we're going to come back to this, probably next time if
24:03 | I time this right, but the answer is, we can do it in n
24:12 | log n time.
24:14 | We're going to come back to that.
24:15 | All right?
24:16 | And I'm going to say-- sort of set that stage here, so that--
24:18 | It turns out that that's probably about the best we can
24:21 | do, or again ends at the length of the list.
24:25 | OK, so that's still comes back to my question.
24:27 | If I want to search a list, should I sort it first and
24:30 | then search it?
24:31 | Hmmm.
24:33 | OK, so let's do the comparison.
24:39 | I'm just going to take an unsorted list and search it, I
24:42 | could do it in linear time, right?
24:43 | One at a time.
24:44 | Walk down the elements until I find it.
24:46 | That would be order n.
24:48 | On the other hand, if I want to sort it first, OK, if I
24:53 | want to do sort and search, I want to sort it, it's going to
24:59 | take n log n time to sort it, and having done that, then I
25:05 | can search it in log n time.
25:09 | Ah.
25:10 | So which one's better?
25:15 | Yeah.
25:20 | Ah-ha.
25:20 | Thank you.
25:21 | Hold on to that thought for second, I'm going to
25:23 | come back to it.
25:23 | That does not assume I'm running a search it wants,
25:25 | which one's better?
25:29 | The unsorted, and you have exactly the point I want to
25:30 | get to-- how come all the guys, sorry, all the people
25:33 | answering questions are way, way up in the back?
25:36 | Wow. that's a Tim Wakefield pitch right there, all right.
25:40 | Thank you.
25:42 | He has it exactly right.
25:43 | OK?
25:45 | Is this smaller than that?
25:48 | No.
25:49 | Now that's a slight lie.
25:50 | Sorry, a slight misstatement, OK?
25:52 | I could run for office, couldn't I, if I can do that
25:54 | kind of talk.
25:55 | It's a slight misstatement in the sense that these should
25:57 | really be orders of growth.
25:58 | There are some constants in there, it depends on the size,
26:00 | but in general, n log n has to be bigger than n.
26:05 | So, as the gentleman back there said, if I'm searching
26:08 | it once, just use the linear search.
26:11 | On the other hand, am I likely to only search a list once?
26:15 | Probably not.
26:16 | There are going to be multiple elements I'm going to be
26:17 | looking for, so that suggests that in fact, I want to
26:24 | amortize the cost.
26:26 | And what does that say?
26:30 | It says, let's assume I want to do k
26:33 | searches of a list. OK.
26:41 | In the linear case, meaning in the unsorted case, what's the
26:44 | complexity of this? k times n, right?
26:48 | Order n to do the search, and I've got to do it k times, so
26:51 | this would be k times n.
26:55 | In the [GARBLED PHRASE]
26:58 | sort and search case, what's my cost?
27:03 | I've got to sort it, and we said, and we'll come back to
27:05 | that next time, that I can do the sort in n log n, and then
27:10 | what's the search in this case?
27:13 | Let's log n to do one search, I want to do k of them, that's
27:17 | k log n, ah-ha!
27:26 | Now I'm in better shape, right?
27:28 | Especially for really large n or for a lot of k, because now
27:31 | in general, this is going to be smaller than that.
27:37 | So this is a place where the amortized cost
27:40 | actually helps me out.
27:41 | And as the gentleman at the back said, the question he
27:43 | asked is right, it depends on what I'm trying to do.
27:46 | So when I do the analysis, I want to think about what am I
27:49 | doing here, am I capturing all the pieces of it?
27:51 | Here, the two variables that matter are what's the length
27:54 | of the list, and how many times I'm going to search it?
27:57 | So in this case, this one wins, whereas in this case,
28:04 | that one wins.
28:07 | OK.
28:08 | Having said that, let's look at doing some sorts.
28:13 | And I'm going to start with a couple of dumb sorting
28:16 | mechanisms. Actually, that's the wrong way saying it,
28:19 | they're simply brain-damaged, they're not dumb, OK?
28:21 | They are computationally challenged, meaning, at the
28:23 | time they were invented, they were perfectly good sorting
28:25 | algorithms, there are better ones, we're going to see a
28:27 | much better one next time around, but this is a good way
28:29 | to just start thinking about how to do the algorithm, or
28:30 | how to do the sort.
28:32 | Blah, try again.
28:33 | How to do this sort.
28:34 | So the first one I want to talk about it's what's called
28:38 | selection sort.
28:40 | And it's on your handout, and I'm going to bring the code up
28:50 | here, you can see it, it's called cell sort, just for
28:53 | selection sort.
28:54 | And let's take a look at what this does.
28:59 | OK.
28:59 | And in fact I think the easy way to look at what this
29:01 | does-- boy.
29:02 | My jokes are that bad.
29:03 | Wow--
29:04 | All right.
29:04 | I think the easiest way to look at what this does, is
29:07 | let's take a really simple example--
29:10 | I want to make sure I put the right things out--
29:20 | I've got a simple little list of values there.
29:23 | And if I look at this code, I'm going to run over a loop,
29:25 | you can see that there, i is going to go from zero up to
29:28 | the length minus 1, and I'm going to keep track of a
29:34 | couple of variables.
29:35 | Min index, I think I called it min val.
29:42 | OK.
29:42 | Let's simulate the code.
29:43 | Let's see what it's doing here.
29:44 | All right, so we start off.
29:47 | Initially i-- ah, let me do it this way, i is going to point
29:53 | there, and I want to make sure I do it right, OK-- and min
29:58 | index is going to point to the value of i, which is there,
30:03 | and min value is initially going to have the value 1.
30:06 | So we're simply catting a hold of what's the first value
30:09 | we've got there.
30:10 | And then what do we do?
30:12 | We start with j pointing here, and we can see what this
30:18 | loop's going to do, right? j is just going to move up.
30:20 | So it's going to look at the rest of the list, walking
30:23 | along, and what does it do?
30:25 | It says, right.
30:27 | If j is-- well it says until j is at the less than the length
30:30 | of l-- it says, if min value is bigger than the thing I'm
30:37 | looking at, I'm going to do something, all right?
30:39 | So let's walk this.
30:40 | Min value is 1,.
30:42 | Is 1 bigger than 8?
30:43 | No.
30:43 | I move j up.
30:44 | Is 1 bigger than 3?
30:44 | No.
30:45 | 1 bigger than 6?
30:46 | No.
30:46 | 1 bigger than 4?
30:47 | No.
30:47 | I get to the end of the loop, and I actually do a little bit
30:50 | of wasted motion there.
30:51 | And the little bit of wasted motion is, I take the value at
30:55 | i, store it away temporarily, take the value where min index
31:00 | is pointing to, put it back in there, and
31:02 | then swap it around.
31:04 | OK.
31:05 | Having done that, let's move i up to here. i is now pointing
31:11 | at that thing.
31:12 | Go through the second round of the loop.
31:13 | OK.
31:14 | What does that say?
31:15 | I'm going to change min index to also point there n value is
31:21 | 8, j starts off here, and I say, OK, is the thing I'm
31:26 | looking at here smaller than that?
31:30 | Yes.
31:31 | Ah-ha.
31:32 | What does that say to do?
31:33 | It says, gee, make min index point to there,
31:41 | min value be 3.
31:44 | Change j.
31:46 | Is 6 bigger than 3?
31:47 | Yes.
31:48 | Is 4 bigger than 3?
31:49 | Yes.
31:50 | Get to the end.
31:51 | And when I get to the end, what do I do?
31:55 | Well, you see, I say, take temp, and store away what's
32:01 | here, all right?
32:04 | Which is that value, and then take what min index is
32:07 | pointing to, and stick it in there, and finally, replace
32:16 | that value.
32:21 | OK.
32:23 | Aren't you glad I'm not a computer?
32:24 | Slow as hell.
32:26 | What's this thing doing?
32:29 | It's walking along the list, looking for the smallest thing
32:34 | in the back end of the list, keeping track of where it came
32:37 | from, and swapping it with that spot in
32:41 | the list. All right?
32:42 | So in the first case, I didn't have to do any swaps because 1
32:45 | was the smallest thing.
32:46 | In the second case, I found in the next smallest element and
32:49 | moved here, taking what was there and moving it on, in
32:52 | this case I would swap the 4 and the 8, and in next case I
32:56 | wouldn't have to do anything.
32:58 | Let's check it out.
32:59 | I've written a little bit of a test script here, so if we
33:02 | test cell sort, and I've written this so that it's
33:07 | going to print out what the list is at the end
33:08 | of each round, OK.
33:13 | Ah-ha.
33:16 | Notice what-- where am I, here-- notice what
33:17 | happened in this case.
33:19 | At the end of the first round, I've got the smallest element
33:22 | at the front.
33:23 | At the end of the second round, I've got the smallest
33:25 | two elements at the front, in fact I got all
33:27 | of them sorted out.
33:29 | And it actually runs through the loop multiple times,
33:31 | making sure that it's in the right form.
33:33 | Let's take another example.
33:36 | OK.
33:39 | Smallest element at the front.
33:40 | Smallest two elements at the front.
33:42 | Smallest three elements at the front.
33:44 | Smallest four elements at the front, you get the idea.
33:46 | Smallest five elements at the front.
33:49 | So this is a nice little search-- sorry, a nice little
33:52 | sort algorithm .
33:53 | And in fact, it's relying on something that we're going to
33:56 | come back to, called the loop invariant.
33:59 | Actually, let me put it on this board so you can see it.
34:16 | The loop invariant what does the loop invariant mean?
34:18 | It says, here is a property that is true of this structure
34:21 | every time through the loop.
34:23 | In the loop invariant here is the following: the list is
34:26 | split, into a prefix or a first part, and a suffix, the
34:37 | prefix is sorted, the suffix is not, and basically, the
34:48 | loop starts off with the prefix being nothing and it
34:50 | keeps increasing the size of the prefix by 1 until it gets
34:53 | through the entire list, at which point there's nothing in
34:55 | the suffix and entire prefix is sorted.
35:00 | OK?
35:01 | So you can see that, it's just walking through it, and in
35:04 | fact if I look at a couple of another-- another couple of
35:06 | examples, it's been a long day, again, you
35:09 | can see that property.
35:12 | You'll also notice that this thing goes through the entire
35:16 | list, even if the list is sorted before it
35:19 | gets partway through.
35:20 | And that you might look at, for example, that first
35:22 | example, and say, man by this stage it was already sorted,
35:25 | yet it had to go through and check that the third element
35:28 | was in the right place, and then the fourth and then the
35:30 | fifth and then the six.
35:32 | OK.
35:34 | What order of growth?
35:35 | What's complexity of this?
35:40 | I've got to get rid of this candy.
35:43 | Anybody help me out?
35:44 | What's the complexity of this?
35:46 | Sorry, somebody at the back.
35:49 | n squared.
35:49 | Yeah, where n is what?
35:52 | Yeah, and I can't even see who's saying that.
35:54 | Thank you.
35:56 | Sorry, I've got the wrong glasses on, but you're
35:57 | absolutely right, and in case the rest of you didn't hear
36:00 | it, n squared.
36:03 | How do I figure that out?
36:05 | Well I'm looping down the list, right?
36:09 | I'm walking down the list. So it's certainly at least linear
36:12 | in the length of the list. For each starting
36:15 | point, what do I do?
36:15 | I look at the rest of the list to decide what's the element
36:19 | to swap into the next place.
36:21 | Now, you might say, well, wait a minute.
36:23 | As I keep moving down, that part gets smaller, it's not
36:26 | always the initial length of the list, and you're right.
36:29 | But if you do the sums, or if you want to think of it this
36:31 | way, if you think about this more generally, it's always on
36:34 | average at least the length of the list. So I've got to do n
36:37 | things n times.
36:39 | So it's quadratic, in terms of that sort.
36:42 | OK.
36:43 | That's one way to do this sort.
36:45 | Let's do another one.
36:50 | The second one we're going to do is called bubble sort.
36:52 | All right?
36:55 | And bubble sort is also on your handout.
36:59 | And you want to take the first of these, let me-- sorry, for
37:07 | a second let me uncomment that, and let me
37:10 | comment this out--
37:11 | All right, you can see the code for bubble sort there.
37:19 | Let's just look at it for a second, then we'll try some
37:21 | examples, and then we'll figure out what
37:23 | it's actually doing.
37:25 | So bubble sort, which is right up here.
37:27 | What's it going to do?
37:28 | It's going to let j run over the length of the list, all
37:32 | right, so it's going to start at some point to move down,
37:34 | and then it's going to let i run over range, that's just
37:38 | one smaller, and what's it doing there?
37:43 | It's looking at successive pairs, right?
37:45 | It's looking at the i'th and the i plus first element, and
37:48 | it's saying, gee, if the i'th element is bigger than the
37:51 | i'th plus first element, what's the next set of three
37:53 | things doing?
37:55 | Just swapping them, right?
37:57 | I temporarily hold on to what's in the i'th element so
37:59 | I can move the i plus first one in, and then replace that
38:02 | with the i'th element.
38:05 | OK.
38:06 | What's this thing doing then, in terms of sorting?
38:08 | At the end of the first pass, what could I say about the
38:13 | result of this thing?
38:16 | What's the last element in the list look like?
38:25 | I hate professors who do this.
38:28 | Well, let's try it.
38:30 | Let's try a little test. OK?
38:35 | Test bubble sort-- especially if I could type-- let's run it
38:40 | on the first list. OK, let's try it on another one.
38:49 | Oops sorry.
38:50 | Ah, I didn't want to do it this time, I forgot to do the
38:53 | following, bear with me.
38:56 | I gave away my punchline.
38:57 | Let's try it again.
38:58 | Test bubble sort.
39:04 | OK, there's the first run, I'm going to take a different
39:07 | list. Can you see a pattern there?
39:18 | Yeah.
39:18 | STUDENT: The last cell in the list is always going to
39:22 | [INAUDIBLE]
39:22 | PROFESSOR ERIC GRIMSON: Yeah.
39:23 | Why?
39:23 | You're right, but why?
39:24 | STUDENT: [UNINTELLIGIBLE PHRASE]
39:28 | PROFESSOR ERIC GRIMSON: Exactly right.
39:29 | Thank you.
39:30 | The observation is, thank you, on the first pass through, the
39:37 | last element is the biggest thing in the list. On the next
39:40 | pass through, the next largest element is at the second point
39:43 | in
39:43 | the list. OK?
39:43 | Because what am I doing?
39:45 | It's called bubble sort because it's literally
39:46 | bubbling along, right?
39:47 | I'm walking along the list once, taking two things, and
39:51 | saying, make sure the biggest one is next.
39:53 | So wherever the largest element started out in the
39:55 | list, by the time I get through it, it's at the end.
39:59 | And then I go back and I start again, and
40:01 | I do the same thing.
40:03 | OK.
40:03 | The next largest element has to end up in
40:05 | the second last spot.
40:06 | Et cetera.
40:07 | All right, so it's called bubble sort because it does
40:09 | this bubbling up until it gets there.
40:12 | Now.
40:14 | What's the order of growth here?
40:15 | What's the complexity?
40:19 | I haven't talked to the side of the room in a while,
40:21 | actually I have. This gentleman has helped me out.
40:23 | Somebody else help me out.
40:23 | What's the complexity here?
40:27 | I must have the wrong glasses on to see a hand.
40:31 | No help.
40:34 | Log?
40:36 | Linear?
40:38 | Exponential?
40:40 | Quadratic?
40:41 | Yeah.
40:43 | Log.
40:44 | It's a good think, but why do you think it's log?
40:50 | Ah-ha.
40:50 | It's not a bad instinct, the length is getting shorter each
40:53 | time, but what's one of the
40:54 | characteristics of a log algorithm?
40:56 | It drops in half each time.
40:58 | So this isn't--
41:00 | OK.
41:01 | And you're also close.
41:02 | It's going to be linear, but how many times do
41:04 | I go through this?
41:05 | All right, I've got to do one pass to bubble the last
41:08 | element to the end.
41:10 | I've got to do another pass to bubble the second last element
41:12 | to the end.
41:12 | I've got to do another pass.
41:14 | Huh.
41:15 | Sounds like a linear number of times I've got to do-- oh
41:19 | fudge knuckle.
41:20 | A linear number of things, quadratic.
41:23 | Right?
41:25 | OK.
41:25 | So this is again an example, this was quadratic, and this
41:32 | one was quadratic.
41:35 | And I have this, to write it out, this is order the length
41:40 | of the list squared, OK?
41:43 | Just to make it clear what we're
41:44 | actually measuring there.
41:48 | All
41:48 | right.
41:48 | Could we do better?
41:50 | Sure.
41:52 | And in fact, next time we're going to show you that n log n
41:54 | algorithm, but even with bubble sort, we can do better.
41:57 | In a particular, if I look at those traces, I can certainly
42:00 | see cases where, man, I already had the list sorted
42:03 | much earlier on, and yet I kept going back to see if
42:06 | there was anything else to bubble up.
42:08 | How would I keep track of that?
42:09 | Could I take advantage of that?
42:12 | Sure.
42:13 | Why don't I just keep track on each pass through the
42:16 | algorithm whether I have done any swaps?
42:18 | All right?
42:20 | Because if I don't do any swaps on a pass through the
42:22 | algorithm, then it says everything's
42:23 | in the right order.
42:24 | And so, in fact, the version that I commented out-- which
42:28 | is also in your handout and I'm now going to uncomment,
42:29 | let's get that one out, get rid of this one-- notice the
42:38 | only change.
42:39 | I'm going to keep track of a little variable called swap,
42:42 | it's initially true, and as long as it's true, I'm going
42:46 | to keep going, but inside of the loop I'm going to set it
42:49 | to false, and only if I do a swap will I set it to true.
42:53 | This says, if I go through an entire pass through the list
42:56 | and nothing gets changed, I'm done.
42:58 | And in fact if I do that, and try test bubble sort, well, in
43:09 | the first case, looks the same.
43:13 | Ah.
43:13 | On the second case, I spot it right away.
43:17 | On the third case, it takes me the same amount of time.
43:20 | And the fourth case, when I set it up, I'm done.
43:24 | OK.
43:24 | So what's the lesson here?
43:25 | I can be a little more careful about keeping track of what
43:28 | goes on inside of that loop.
43:30 | If I don't have any more work to do, let me just stop.
43:31 | All right.
43:33 | Nonetheless, even with this change, what's the order
43:36 | growth for bubble sort?
43:39 | Still quadratic, right?
43:40 | I'm looking for the worst case behavior, it's still
43:42 | quadratic, it's quadratic in the length of the list, so I'm
43:44 | sort of stuck with that.
43:47 | Now.
43:47 | Let me ask you one last question, and then
43:49 | we'll wrap this up.
43:51 | Which of these algorithms is better?
43:55 | Insertion sort or bubble sort?
43:57 | STUDENT: Bubble.
43:59 | PROFESSOR ERIC GRIMSON: Bubble.
43:59 | Bubble bubble toil and trouble.
44:00 | Who said bubble?
44:01 | Why?
44:02 | STUDENT: Well, the first one was too inefficient
44:04 | [UNINTELLIGIBLE] store and compare each one, so
44:07 | [UNINTELLIGIBLE]
44:15 | PROFESSOR ERIC GRIMSON: It's not a bad instinct.
44:16 | Right.
44:16 | So it-- so, your argument is, bubble is better because it's
44:19 | is essentially not doing all these extra comparisons.
44:23 | Another way of saying it is, I can do this stop when
44:25 | I don't need to.
44:25 | All right?
44:26 | OK.
44:28 | Anybody have an opposing opinion?
44:30 | Wow, this sounds like a presidential debate.
44:34 | Sorry, I should reward you.
44:35 | Thank you for that statement.
44:37 | Anybody have an opposing opinion?
44:40 | Everybody's answering these things and sitting
44:41 | way up at the back.
44:42 | Nice catch.
44:44 | Yeah.
44:44 | STUDENT: [INAUDIBLE]
44:55 | PROFESSOR ERIC GRIMSON: I don't think so, right?
44:55 | I think selection sort, I still have to go through
44:57 | multiple times, it was still quadratic, OK, but I think
45:01 | you're heading towards a direction I want to get at, so
45:03 | let me prime this a little bit.
45:05 | How many swaps do I do in general in bubble sort,
45:10 | compared to selection source?
45:13 | God bless.
45:14 | Oh, sorry, that wasn't a sneeze, it was a two?
45:18 | How many swaps do I do in bubble sort?
45:23 | A lot.
45:24 | Right.
45:24 | Potentially a lot because I'm constantly doing that, that
45:27 | says I'm running that inner loop a whole bunch of times.
45:29 | How many swaps do I do in selection sort?
45:34 | Once each time.
45:36 | Right?
45:36 | I only do one swap potentially, it-- though not
45:39 | one potentially, each time at the end of
45:40 | the loop I do a swap.
45:42 | So this actually suggests again, the orders of growth
45:45 | are the same, but probably selection sort is a more
45:49 | efficient algorithm, because I'm not doing that constant
45:51 | amount of work every time around.
45:53 | And in fact, if you go look up, you won't see bubble sort
45:55 | used very much.
45:56 | Most--
45:57 | I shouldn't say most, many computer scientists don't
45:59 | think it should be taught, because it's just so
46:00 | inefficient.
46:01 | I disagree, because it's a clever idea, but it's still
46:03 | something that we have to keep track of.
46:06 | All right.
46:07 | We haven't gotten to our n log n algorithm, we're going to do
46:10 | that next time, but I want to set the stage here by pulling
46:14 | out one last piece.
46:16 | OK.
46:17 | Could we do better in terms of sorting?
46:19 | Again, remember what our goal was.
46:20 | If we could do sort, then we saw, if we amortized the cost,
46:23 | that searching is a lot more efficient if we're searching a
46:25 | sorted list.
46:27 | How could we do better?
46:29 | Let me set the stage.
46:30 | I already said, back here, when I used this board, that
46:34 | this idea was really important.
46:36 | And that's because that is a version of a divide and
46:43 | conquer algorithm.
46:48 | OK.
46:48 | Binary search is perhaps the simplest of the divide and
46:51 | conquer algorithms, and what does that mean?
46:53 | It says, in order to solve a problem, cut it down to a
46:56 | smaller problem and try and solve that one.
46:58 | So to just preface what we're going to do next time, what
47:01 | would happen if I wanted to do sort, and rather than in
47:04 | sorting the entire list at once, I broke it into pieces,
47:09 | and sorted the pieces, and then just figured out a very
47:12 | efficient way to bring those two pieces and merge them back
47:15 | together again?
47:16 | Where those pieces, I would do the same thing with, I would
47:19 | divide them up into smaller chunks, and sort those.
47:23 | Is that going to give me a more efficient algorithm?
47:25 | And if you come back on Thursday,
47:27 | we'll answer that question.
